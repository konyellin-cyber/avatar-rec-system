# 架构设计

## 演进历史

### 2026-02-20: 双塔召回架构（初版）

**核心思路**: 将记忆召回问题类比为推荐系统的双塔召回

**三阶段设计**:

```
Stage 1: Query Understanding (User Tower)
→ LLM Query Rewrite: 指代消解 + 意图推断 + 实体提取

Stage 2: Memory Retrieval (Dual Tower Recall)  
→ 多路召回: 实体/时间/向量/图
→ 多路融合: 加权打分

Stage 3: Context Ranking & Injection
→ LLM 重排: 理解连贯性，选择最优上下文组合
→ 注入 LLM Context Window
```

**核心洞察**:
> "结合推荐系统，这个过程非常像双塔召回系统，但是 user 端的向量，需要更好的理解，用来检索记忆这个塔，来提供给 LLM 一个更好的上下文。" - plancklin, 2026-02-20

**推荐系统经验迁移**:
1. **多路召回融合**: U2I/I2I/热门/规则 → Query-Memory/Memory-Memory/高频/状态
2. **精排多目标**: CTR×α + CVR×β + 时长×γ → 相关性×α + 时间×β + 重要性×γ + 连贯性×δ
3. **探索与利用**: 精准召回核心记忆（Exploitation） + 关联记忆启发视角（Exploration）
4. **序列建模**: 用户行为序列预测兴趣 → 对话序列理解决策路径，预测意图
5. **用户画像层次**: 实时/短期/长期/静态 → 当前对话/最近3天/MEMORY.md/USER.md

---

### 2026-02-21: 架构调整 - 两层检索模型

**核心反思**:
> "记忆召回的双塔结构存在一些问题，对于个人的记忆系统，其实没有必要用大量的数据去监督学习" - plancklin, 2026-02-21

**问题识别**:
1. 个人记忆只有**千级数据**，没有足够的监督信号
2. 双塔召回需要**海量数据**的监督学习
3. 盲目套用推荐系统架构 = **过度设计**

**新架构思路**:
> "我觉得他分为两部分：
> 1. 对每个对话在记忆中的**快速检索** - 既要节省 token，也需要加快速度
> 2. 对问题进一步的**海量检索** - 比如推荐系统中的海量发表数据，进一步给出更好的回答"

---

## 当前架构：两层检索模型

### 第一层：快速记忆检索

**定位**: 高频、轻量的个人记忆召回

**数据规模**: 千级（个人记忆）

**触发频率**: 每次对话

**核心约束**:
- ⚠️ **Token 效率**: 不能把所有记忆都塞进 Context（预算 <1000 tokens）
- ⚠️ **速度**: 秒级响应，不能让用户等太久（<2秒）
- ⚠️ **准确性**: 找到最相关的 5-10 条记忆

#### 技术方案：规则召回 + 轻量排序

**为什么不用双塔？**
- 双塔的优势是**速度**（离线编码 + 向量检索）
- 但个人记忆系统不需要这个速度（千级数据，不是百亿级）
- 需要的是**深度理解**，而非浅层向量匹配

**召回策略**（多路并行）:

```
1. 时间路径: 最近 3 天日志（~10条）
2. 标签路径: 关键词 → 标签映射（~5条）
3. 文件路径: insights.md / projects.md 优先（~3条）
4. 文本搜索: ripgrep 全文检索（~5条）
```

**排序策略**（加权打分）:

```python
score = 时间权重×0.3 + 标签匹配×0.4 + 文件优先级×0.3
```

**输出**: Top-K 记忆片段（K=5, max_tokens=1000）

**特点**:
- ✅ 不需要训练数据
- ✅ 可解释（能看到为什么选这些记忆）
- ✅ 低延迟（<2秒）
- ✅ 动态上下文感知（结合当前对话）

---

### 第二层：海量知识检索

**定位**: 低频、深度的外部知识召回

**数据规模**: 亿级（论文/网页/内网文档/讨论）

**触发频率**: 按需触发（用户明确要求深度回答时）

**核心约束**:
- 📊 **深度**: 全面、可信、有引用
- 📊 **全面性**: 多数据源融合
- 可以接受较长延迟（<30秒）

#### 技术方案：双塔召回 + LLM 精排

**这里才需要推荐系统的能力！**

**Stage 1: Query Understanding (User Tower)**

```
LLM Query Rewrite:
- 指代消解: "那个算法" → "双塔召回算法"
- 意图推断: "有什么论文" → 学术文献检索
- 实体提取: "推荐系统" + "召回" + "深度学习"
- 生成 Query Vector
```

**Stage 2: Multi-Source Recall (Item Tower)**

```
多路召回（并行）:
├─ 个人记忆召回（query_vector）
├─ ArXiv 论文召回（query_vector）
├─ 网页搜索召回（entities）
├─ iSearch 内网文档（entities）
└─ 社交讨论召回（entities, 小红书/微博）

召回数量: 每路 Top-20，共 ~100 条候选
```

**Stage 3: LLM Re-ranking & Synthesis**

```
LLM 深度理解:
- 读取所有候选内容
- 理解与 Query 的相关性
- 评估可信度、时效性、深度
- 排序 + 选择 Top-10

答案合成:
- 综合多个来源
- 提供引用和出处
- 给出可执行建议
```

**输出**: 深度回答 + 引用来源

**特点**:
- ✅ 处理海量数据（双塔向量召回）
- ✅ 深度理解（LLM 精排）
- ✅ 多数据源融合
- ✅ 可信度保证（有引用）

---

## 两层对比

| 维度 | 第一层（快速记忆） | 第二层（海量检索） |
|------|------------------|------------------|
| **数据源** | 个人记忆（千级） | 外部知识（亿级） |
| **触发频率** | 每次对话 | 按需触发 |
| **延迟要求** | <2秒 | <30秒 可接受 |
| **Token 成本** | 严格控制（<1000） | 可以较高 |
| **召回策略** | 规则 + 简单排序 | 双塔召回 + 精排 |
| **目标** | 上下文注入 | 深度回答 |
| **是否需要训练** | ❌ 不需要 | ✅ 需要（外部数据） |

---

## 为什么要分两层？

### 规模与深度的平衡

**核心矛盾**:
- 推荐系统的双塔优化的是**速度**（离线编码）
- 记忆系统需要的是**深度理解**（在线理解）

**解决方案**:
- 第一层：数据少，不需要双塔的速度优势，用规则召回即可
- 第二层：数据多，需要双塔的召回能力，否则无法处理亿级候选

### Token 经济性

**第一层的 Token 预算紧张**:
- 每次对话都触发
- 必须严格控制在 1000 tokens 内
- 不能用 LLM 做召回（太慢 + 太贵）

**第二层的 Token 预算宽松**:
- 低频触发（可能一天几次）
- 可以让 LLM 深度理解和排序
- 成本可接受

---

## 类比：搜索引擎的演进

**传统搜索（Google 早期）**:
```
关键词匹配 → PageRank 排序 → 返回链接
```

**现代搜索（Perplexity / ChatGPT Search）**:
```
关键词召回 → LLM 阅读候选页面 → 理解意图 → 合成答案
```

**个人记忆召回应该学谁？**
- ❌ 不应该学 Google（双塔向量召回）
- ✅ 应该学 Perplexity（规则召回 + LLM 深度理解）

**外部知识检索应该学谁？**
- ✅ 学 Google（双塔召回处理海量数据）
- ✅ 也学 Perplexity（LLM 深度理解和合成）

---

## 待验证问题

### 第一层（快速记忆）

**Q1: 触发时机**
- 每次对话开始时自动检索？
- 还是用户问到时再去检索？

**Q2: Token 预算和速度**
- Token 预算: < 1000 tokens？
- 速度要求: < 2 秒？
- 这两个约束中，哪个更重要？

**Q3: 输出形式**
- 直接注入到 Context Window？
- 还是先给用户看，用户选择要不要用？

**Q4: 召回策略**
- 四路召回的权重如何调整？
- 是否需要根据 Query 类型动态调整？
- 如何处理"找不到相关记忆"的情况？

---

### 第二层（海量检索）

**Q5: 数据源定义**
- "海量发表数据"具体指什么？
- ArXiv、网页、内网文档、社交讨论？
- 各数据源的权重如何分配？

**Q6: 触发条件**
- 第一层检索不到有用信息时？
- 用户明确要求深度回答时？
- 还是每次都执行，只是后台慢慢做？

**Q7: 评价标准**
- "更好的回答"指的是？
- 更全面（覆盖更多信息源）？
- 更深刻（结合外部知识的洞察）？
- 更可信（有引用和来源）？

**Q8: Query Tower 复杂度**
- 全程 LLM（理解能力强，但慢）？
- 轻量模型（快，但理解能力弱）？
- 延迟和成本如何平衡？

---

### 整体架构

**Q9: 两层交互**
- 第一层失败时如何优雅降级到第二层？
- 第二层结果如何反哺第一层（记忆更新）？

**Q10: 冷启动**
- 新用户/新话题时的兜底策略？
- 如何快速建立初始记忆？

**Q11: 业务迁移**
- 能否应用到公众号推荐的创作者理解场景？
- 哪些部分可以复用，哪些需要重新设计？

---

## 实现优先级

### Phase 1: 第一层原型（当前）

**目标**: 验证规则召回的有效性

**任务**:
- [ ] 实现四路召回逻辑
- [ ] 实现轻量排序算法
- [ ] 测试 Token 效率和速度
- [ ] 收集用户反馈

**成功标准**:
- 召回准确率 > 80%（用户认为相关）
- 延迟 < 2 秒
- Token 消耗 < 1000

---

### Phase 2: 第二层设计

**目标**: 明确数据源和触发条件

**任务**:
- [ ] 明确"海量数据"的具体定义
- [ ] 设计触发条件和用户交互
- [ ] 选择/训练双塔模型（如果需要）
- [ ] 设计 LLM 精排策略

**成功标准**:
- 回答深度显著提升（用户主观评价）
- 引用来源可信
- 延迟可接受（<30秒）

---

### Phase 3: 主动对话

**目标**: 基于记忆主动发起讨论

**任务**:
- [ ] 设计主动对话的时机选择算法
- [ ] 实现话题推荐逻辑
- [ ] 测试用户接受度
- [ ] 迭代优化

**成功标准**:
- 主动对话被接受率 > 60%
- 用户认为"有价值"的比例 > 80%

---

_下一步: [关键讨论](03-discussions.md) - 回顾演进过程_
