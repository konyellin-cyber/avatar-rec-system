# 推荐系统

Recommendation Systems

---

## <a name="two-tower"></a>Two-Tower Structure Recommendation

**⭐ 优先级: P2 | 相关性: ⭐⭐⭐**

### 基本信息
- **发表**: Electronics (MDPI), 2025-03-02
- **链接**: https://www.mdpi.com/2079-9292/14/5/1003

### 核心观点
双塔模型是推荐系统召回阶段的核心架构，设计理念源于 DSSM（Deep Structured Semantic Model）。

### 双塔架构

```
┌─────────────┐        ┌─────────────┐
│  User Tower │        │  Item Tower │
│             │        │             │
│  用户特征    │        │  物品特征    │
│     ↓       │        │     ↓       │
│  DNN编码    │        │  DNN编码    │
│     ↓       │        │     ↓       │
│  User Vec   │        │  Item Vec   │
└──────┬──────┘        └──────┬──────┘
       │                      │
       └─────────┬────────────┘
                 ↓
            向量内积/余弦
                 ↓
            相似度分数
```

### 核心优势

#### 1. 离线编码
- User/Item 向量可以**离线**预计算
- 线上只需计算向量相似度（毫秒级）
- 适合**亿级**候选集的快速召回

#### 2. 泛化能力
- 通过监督学习捕捉复杂的用户-物品匹配模式
- 可以处理稀疏特征和冷启动问题

#### 3. 可扩展性
- 支持多路召回融合
- 可以轻松扩展到多模态特征

### 训练目标
- **点击率预测 (CTR)**: 用户是否会点击物品
- **交互预测**: 用户-物品的匹配度
- **损失函数**: 交叉熵、Triplet Loss 等

### 对本项目的启发

#### ⚠️ 适用性分析

| 维度 | 推荐系统 | 个人记忆系统 |
|------|---------|-------------|
| **数据规模** | 亿级 | 千级 |
| **训练数据** | 数百万交互记录 | 无监督信号 |
| **更新频率** | 每天/每周重训 | 实时增量 |
| **召回延迟** | 毫秒级 | 秒级可接受 |

#### ✅ 适用场景（第二层：海量检索）
当需要从**外部知识库（亿级）**中检索时：
- 可以使用预训练的双塔模型（如 BERT-based）
- 离线编码所有外部文档
- 在线快速召回 Top-K

#### ❌ 不适用场景（第一层：快速记忆检索）
个人记忆只有**千级**数据：
- 没有足够的监督信号训练双塔
- 规则召回 + 轻量排序更高效
- 不需要毫秒级的响应速度

### 实现建议

#### 第二层使用双塔（外部知识库）
```python
# 使用预训练模型
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')

# 离线编码外部文档
doc_embeddings = model.encode(external_docs)

# 在线检索
query_embedding = model.encode(user_query)
similarities = cosine_similarity(query_embedding, doc_embeddings)
top_k = np.argsort(similarities)[-k:]
```

#### 第一层使用规则召回（个人记忆）
- 时间窗口筛选
- 标签匹配
- 关键词检索
- 简单的 TF-IDF 排序

---

## DSSM (Deep Structured Semantic Model)

**⭐ 优先级: P2 | 相关性: ⭐⭐⭐**

### 基本信息
- **提出**: Microsoft Research, 2013
- **论文**: "Learning Deep Structured Semantic Models for Web Search"

### 历史地位
- 双塔架构的**鼻祖**
- 首次将深度学习引入语义匹配

### 核心思想
将 Query 和 Document 分别映射到同一个语义空间，通过向量相似度衡量匹配度。

### 对推荐系统的影响
- YouTube 推荐（2016）
- 阿里巴巴电商推荐
- 字节跳动内容推荐
- 微信看一看、视频号

### 对本项目的启发
- 💡 双塔思想的本质是**语义匹配**
- 💡 在有监督数据的场景下，双塔是强大的
- 💡 在无监督场景下，需要其他方案

---

## 多路召回融合

**⭐ 优先级: P2 | 相关性: ⭐⭐⭐**

### 核心观点
工业界推荐系统通常采用**多路召回**策略，而非单一方法。

### 典型召回路径

| 召回路径 | 原理 | 特点 | 使用场景 |
|---------|------|------|---------|
| **U2I（User-to-Item）** | 用户兴趣 → 物品 | 个性化强 | 日常推荐 |
| **I2I（Item-to-Item）** | 物品相似 → 物品 | 相关性高 | "看了又看" |
| **热门召回** | 全局热度 | 覆盖率高 | 冷启动 |
| **规则召回** | 人工规则 | 可控性强 | 运营需求 |
| **图召回** | 关系图谱 | 发现潜在关联 | 探索推荐 |

### 融合策略
1. **并行召回**: 各路独立召回，汇总去重
2. **分数融合**: 加权求和或学习融合权重
3. **分层召回**: 第一层粗排，第二层精排

### 对本项目的启发

✅ **第一层（个人记忆）也可以多路召回**

| 召回路径 | 原理 | 实现 |
|---------|------|------|
| **时间召回** | 最近的记忆 | 最近 3 天优先 |
| **标签召回** | 主题标签匹配 | insights/decisions/projects |
| **关键词召回** | 文本匹配 | TF-IDF 或关键词 |
| **向量召回** | 语义相似 | 使用轻量向量模型 |
| **图召回** | 记忆关联 | Memory-to-Memory 关系 |

✅ **融合原则**
- 不同路径覆盖不同类型的需求
- 避免单一路径的盲区
- 通过多样性提升召回质量

---

## 推荐系统的精排（Ranking）

**⭐ 优先级: P2 | 相关性: ⭐⭐⭐**

### 召回 vs 精排

| 阶段 | 候选规模 | 复杂度 | 目标 |
|------|---------|--------|------|
| **召回** | 百万 → 千 | 轻量 | 保证覆盖 |
| **精排** | 千 → 十 | 重模型 | 保证精准 |

### 精排模型特点
- 特征丰富（用户 × 物品 × 上下文）
- 模型复杂（DNN、GBDT、Transformer）
- 目标多样（CTR、CVR、时长、互动）

### 多目标优化
```
Score = α × CTR + β × CVR + γ × Duration + δ × Interaction
```

### 对本项目的启发

✅ **第一层的 Ranking**（个人记忆）

可能的排序因子：
- **相关性**: 与当前查询的匹配度
- **时间衰减**: 越近的记忆权重越高
- **重要性**: insights > decisions > conversations > 日志
- **连贯性**: 与当前对话主题的连续性

排序公式：
```
Score = α × Relevance + β × TimeDecay + γ × Importance + δ × Coherence
```

✅ **第二层的 Ranking**（外部知识库）

- 相关性（语义匹配）
- 权威性（来源可信度）
- 时效性（发表时间）
- 全面性（内容覆盖度）

---

## 探索 vs 利用（Exploration vs Exploitation）

**⭐ 优先级: P2 | 相关性: ⭐⭐⭐**

### 核心问题
推荐系统需要在**精准推荐**（利用已知偏好）和**探索发现**（拓展未知兴趣）之间平衡。

### 经典策略

#### ε-greedy
- (1-ε) 的概率选择最优项
- ε 的概率随机探索

#### Thompson Sampling
- 基于贝叶斯推断的动态平衡

#### UCB (Upper Confidence Bound)
- 选择"期望 + 不确定性"最高的项

### 对本项目的启发

✅ **记忆召回也需要平衡**
- **精准召回**: 确保高相关性的核心记忆被召回
- **关联探索**: 偶尔召回"不那么直接但可能启发的"记忆

✅ **实现方式**
- Top-K 中，前 80% 是高相关记忆
- 后 20% 是相关度略低但可能有启发的记忆
- 用户可以选择是否展开探索

---

## 待补充论文

- [ ] YouTube 推荐系统（双塔召回 + DNN 精排）
- [ ] 阿里巴巴电商推荐架构
- [ ] 字节跳动推荐系统
- [ ] 微信看一看/视频号推荐
- [ ] Collaborative Filtering 经典论文

---

_最后更新: 2026-02-21_
